# LLM Provider (ollama or groq)
LLM_PROVIDER=ollama

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=llama3:8b
GENERATIVE_MODEL=llama3:8b
REVIEW_MODEL=llama3:8b
SUMMARY_MODEL=llama3:8b
COMPARE_MODEL=llama3:8b

# Groq Configuration
GROQ_API_KEY=gsk_Y5WKXs7cf9rb9TcVQ9AYWGdyb3FY9E6osG5iKRMcyTUFBPdfhrir55
GROQ_DEFAULT_MODEL=llama3-70b-8192
GROQ_GENERATIVE_MODEL=llama3-70b-8192
GROQ_REVIEW_MODEL=llama3-70b-8192
GROQ_SUMMARY_MODEL=llama3-70b-8192
GROQ_COMPARE_MODEL=llama3-70b-8192

# Common LLM Settings
GENERATIVE_TEMPERATURE=0.7
REVIEW_TEMPERATURE=0.7
SUMMARY_TEMPERATURE=0.7
COMPARE_TEMPERATURE=0.7
REASONING_MODE=true
REASONING_TEMPERATURE=0.1

# GPU Settings (for Ollama)
ENABLE_GPU=true
GPU_LAYERS=-1  # Use all available GPU layers


#llama3.2:1b
#llama3:8b